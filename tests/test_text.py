import pytest
from src.lib.text import normalize, tokenize, count_freq, top_n


@pytest.mark.parametrize(
    "source, expected",
    [
        ("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("—ë–∂–∏–∫, –Å–ª–∫–∞", "–µ–∂–∏–∫, –µ–ª–∫–∞"),
        ("Hello\r\nWorld", "hello world"),
        ("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"),
        ("", ""),
        ("   ", ""),
        ("–û–¥–∏–Ω", "–æ–¥–∏–Ω"),
        ("–ú–ù–û–ì–û\t\t\t–ü–†–û–ë–ï–õ–û–í\n\n\n", "–º–Ω–æ–≥–æ –ø—Ä–æ–±–µ–ª–æ–≤"),
    ],
)
def test_normalize_basic(source, expected):
    assert normalize(source) == expected


@pytest.mark.parametrize(
    "source, expected",
    [
        ("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]),
        ("hello,world!!!", ["hello", "world"]),
        ("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]),
        ("2025 –≥–æ–¥", ["2025", "–≥–æ–¥"]),
        ("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]),
        ("", []),
        ("   ", []),
        ("word1_word2", ["word1_word2"]),
        ("test-test", ["test-test"]),
        ("a-b-c", ["a-b-c"]),
        ("-–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ", ["–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ"]),
        ("–ø—Ä–∞–≤–∏–ª—å–Ω–æ-", ["–ø—Ä–∞–≤–∏–ª—å–Ω–æ"]),
    ],
)
def test_tokenize_basic(source, expected):
    assert tokenize(source) == expected


@pytest.mark.parametrize(
    "tokens, expected",
    [
        (["a", "b", "a", "c", "b", "a"], {"a": 3, "b": 2, "c": 1}),
        ([], {}),
        (["word"], {"word": 1}),
        (["a", "a", "a"], {"a": 3}),
        (["one", "two", "three"], {"one": 1, "two": 1, "three": 1}),
    ],
)
def test_count_freq_basic(tokens, expected):
    assert count_freq(tokens) == expected


@pytest.mark.parametrize(
    "freq_map, n, expected",
    [
        ({"a": 3, "b": 2, "c": 1}, 2, [("a", 3), ("b", 2)]),
        ({"a": 3, "b": 2, "c": 1}, 5, [("a", 3), ("b", 2), ("c", 1)]),
        ({"word": 1}, 1, [("word", 1)]),
        ({}, 5, []),
        ({"a": 1, "b": 1, "c": 1}, 2, [("a", 1), ("b", 1)]),
    ],
)
def test_top_n_basic(freq_map, n, expected):
    assert top_n(freq_map, n) == expected


def test_top_n_tie_breaker():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É –ø—Ä–∏ —Ä–∞–≤–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö —á–∞—Å—Ç–æ—Ç—ã"""
    freq_map = {"bb": 2, "aa": 2, "cc": 3, "dd": 2}
    result = top_n(freq_map, 4)
    assert result == [("cc", 3), ("aa", 2), ("bb", 2), ("dd", 2)]


def test_top_n_all_same_frequency():
    """–í—Å–µ —Å–ª–æ–≤–∞ —Å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —á–∞—Å—Ç–æ—Ç–æ–π - —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É"""
    freq_map = {"zebra": 1, "apple": 1, "banana": 1}
    result = top_n(freq_map, 3)
    assert result == [("apple", 1), ("banana", 1), ("zebra", 1)]


def test_top_n_default_n():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é n=5"""
    freq_map = {"a": 1, "b": 2, "c": 3, "d": 4, "e": 5, "f": 6}
    result = top_n(freq_map)
    assert len(result) == 5
    assert result[0] == ("f", 6)
